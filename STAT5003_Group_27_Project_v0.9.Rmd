---
title: "Computational Statistics Final Project"
author: "Group 27"
date: "14 October 2018"
output:
  html_document:
    toc: true
    toc_depth: 3
    toc_float: true
---
#Kinase-substrate prediction using phosphoproteomics datasets
Problem Statement: Humphrey et al. (2013) conducted time-course phosphoproteome profiling of insulin stimulated fat cells (3T3-L1). The resulting data can be used to uncover unknown aspects of insulin pathways which is important for treatment of type II diabetes. Akt and mTOR are believed to be central kinases involved in the insulin signalling fat cells. Kinases regulate their substrates by recognising substrate peptide sequence motif. Substrates of the same kinase often have similar response profiles. This similarity in response profiles can be leveraged to predict novel substrates of Akt and mTOR by using learning features from temporal phosphoproteomics data.

Aim: Prediction is a central application in many real-world data analyses. In this project, we will
aim to apply classification techniques for predicting novel kinase-substrates.

Objective: Design a predictive model for identifying novel Akt and mTOR substrates using the InsulinPhospho.txt, AUC_Ins.txt, Akt_substrates.txt and mTOR_substrates.txt datasets.

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# 1. Import all required libraries for analysis
```{r import libraries}
library(ggplot2)
library(mlbench)
library(randomForest)
library(caret)
library(scatterplot3d)
library(MASS)
library(gbm)
library(class)
library(dplyr)
library(Hmisc)
library(xgboost)
library(MLmetrics)
library(mice)
library(tidyverse)
library(corrplot)
library(e1071)
library(scales)
library(stringr)
library(reshape)
library(car)
library(plotly)
library(caret)
source("3_RebuiltAdasamplingFunctions.R")
source("functions.R")
source("3_RebuiltAdasamplingFunctions_SVM.R")


```


# 2. Import required datasets for analysis. 
```{r Import datasets}
InsulinPhospho <- read.csv("./Data2/Datasets/InsulinPhospho.txt", sep="\t") # Main temporal phosphoproteomics data.
AUC_Ins <- read.csv("./Data2/Datasets/AUC_Ins.txt", sep="\t") # Secondary temporal phosphoproteomics data.

Akt_Substrates <- read.csv("./Data2/Datasets/Akt_substrates.txt", header = F) # Identifiers of known Akt substrates.
mTOR_Substrates <- read.csv("./Data2/Datasets/mTOR_substrates.txt", header = F) # Identifiers of known mTOR substrates.

# Name the column of Akt and mTOR dataframes.
names(Akt_Substrates) <- "Identifier"
names(mTOR_Substrates) <- "Identifier"
```


# 3. Data Exploration, Visualisation and Preparation
## 3.1 Summary, describe and dim functions.
```{r}
# Summary of InsulinPhospho
summary(InsulinPhospho)
describe(InsulinPhospho)
dim(InsulinPhospho)

# Summary of AUC_Ins
summary(AUC_Ins)
describe(AUC_Ins)
dim(AUC_Ins)

# Summary of Akt_Substrates
summary(Akt_Substrates)
describe(Akt_Substrates)
dim(Akt_Substrates)

# Summary of mTOR_Substrates
summary(mTOR_Substrates)
describe(mTOR_Substrates)
dim(mTOR_Substrates)
```

## 3.2 Examination of AUC_Ins
Both temporal phosphoproteomics dataframe contain the same number of rows (samples), with different numbers of columns (features). Check to see that the secondary temporal phosphoproteomics dataframe (AUC_Ins) contains different data to InsulinPhospho.
```{r}
test <- AUC_Ins %>% merge(InsulinPhospho, by="Identifier") # Merge the dataframes using their unique identifier to join on.

# Create a dataframe where AUC_Ins features = InsulinPhospho features.
test.result <- test %>% dplyr::select(Identifier, Seq.Window.x, Seq.Window.y, AUC.x, AUC.y) %>% filter(Seq.Window.x == Seq.Window.y | AUC.x == AUC.y) 

head(test.result)

write.csv(test.result, file = "./Output/AUC_Ins_test_result.csv")

dim(test.result) # test has 12,062 rows (the same number of rows as InsulinPhospho). This shows that AUC_Ins contains no new data.

rm(test.result) #drop the test data.frame since we don't actually need it.

# There is no need to consider data from AUC_Ins because it is contained within InsulinPhospho.
```

## 3.3 Create a combined master data set
Identifiers for known Akt_substrates and mTOR_substrates are found in Akt_substrates and mTOR_substrates data sets. Append known substrates fields to InsulinPhospho columns to create a master data set.
```{r}
dat <- InsulinPhospho #Name master data set.

# Add Akt Substrates to dat
dat$aktSubstrate <- 0 # Add new column of zeros to dat for aktSubstrate
dat$aktSubstrate[dat$Identifier %in% Akt_Substrates$Identifier] <- 1 # Label rows with known AktSubstrates with a 1.
dat$aktSubstrate <- as.factor(dat$aktSubstrate) # Change labels to factors.
dat %>% filter(aktSubstrate == 1) # There should be 22 Akt Substrates. Check to make sure this is the case.

# Repeating the same process as above for mTOR substrates:
dat$mTORSubstrate <- 0
dat$mTORSubstrate[dat$Identifier %in% mTOR_Substrates$Identifier] <- 1
dat %>% filter(mTORSubstrate == 1) # There should be 26 Akt Substrates. Check to make sure this is the case.
dat$mTORSubstrate <- as.factor(dat$mTORSubstrate)

# Check for NAs in the master dataset, to identify whether imputation or field/record removal is required
sum(is.na(dat)) # No NA's in dataset. No imputation or data removal required.
```

## 3.4 Visualisation 
A series of visualisations were created to better understand the data:<br/>
a) Visualise the data to understand patterns between the features, and identify potential new features.<br/>
b) Visualise the log2 time-series insulin stimulated phosphorylation profiles fold change.<br/>
c) Scatter plots<br/>
d) Violin plots<br/>
e) kernel density plots


```{r}

# Create a dataframe of the time series data, renaming the time columns to allow for ordering.
dat.timeseries <- dat
dat.timeseries <- dat.timeseries %>% dplyr::rename("1" = "X15s")
dat.timeseries <- dat.timeseries %>% dplyr::rename("2" = "X30s")
dat.timeseries <- dat.timeseries %>% dplyr::rename("3" = "X1m")
dat.timeseries <- dat.timeseries %>% dplyr::rename("4" = "X2m")
dat.timeseries <- dat.timeseries %>% dplyr::rename("5" = "X5m")
dat.timeseries <- dat.timeseries %>% dplyr::rename("6" = "X10m")
dat.timeseries <- dat.timeseries %>% dplyr::rename("7" = "X20m")
dat.timeseries <- dat.timeseries %>% dplyr::rename("8" = "X60m")

# Use melt to stack all the time-series data into a single column for plotting. Order dataframe by Identifier.
my.result <- melt(dat.timeseries, id = c("Identifier","Seq.Window","Avg.Fold", "AUC","Ins.1","LY","Ins.2","MK","aktSubstrate","mTORSubstrate")) 
my.result <- my.result[order(my.result$Identifier),]

# The time-series profiles start at different y values. Adjust all y values to start at zero. This allows for an "apples to apples comparison" of the time-series.
my.result.var1 <- my.result %>% filter(variable == 1) %>% select(Identifier, value)
my.result2 <- my.result %>% merge(my.result.var1, by="Identifier", all.x=TRUE) %>% mutate(value.offset = value.x - value.y)

```

### 3.4.1 Time-series response curves for Akt and mTOR
Plot the time-series response curves for Akt and mTOR to examine differences in their profiles.
```{r}

ggplot(my.result2 %>% filter(aktSubstrate==1 | mTORSubstrate == 1))+
  geom_line(aes(x=variable, y=value.offset, colour = aktSubstrate, group=Identifier))
```

A notable difference can be seen between the Akt (1) and mTOR (0) response curves between 1 (time = 15s) and 6 (time=10mins). Beyond 6 (time=10mins) both response curse appear to have a similar flat/negative response.

### 3.4.2 Replot Time-series response curves using mean of Akt and mTOR
Re-plot the above graph taking the mean of Akt and mTOR. Include error bars of 1 standard deviation.
```{r}

# Use group-by to combine Akt and mTOR samples at each time interval and calculate the mean and standard deviation.
my.result2.grouped <- my.result2 %>% group_by(aktSubstrate, mTORSubstrate, variable) %>% dplyr::summarize(value.offset.mn = mean(value.offset), value.offset.var = var(value.offset)) %>% data.frame %>% mutate(value.offset.sd = sqrt(value.offset.var))

# Filter for rows containing know Akt and mTOR substrates (i.e. remove all unknown rows)
my.result2.grouped
my.result2.grouped %>% filter(aktSubstrate == 1 | mTORSubstrate == 1)

# Plot mean response including 1 standard deviation bar.
ggplot(my.result2.grouped %>% filter(aktSubstrate==1 | mTORSubstrate == 1))+
  geom_point(aes(x=variable, y=value.offset.mn, colour = aktSubstrate, group=aktSubstrate)) +
  geom_line(aes(x=variable, y=value.offset.mn, colour = aktSubstrate, group=aktSubstrate)) +
  geom_errorbar(width=.4, aes(x=variable,ymin=value.offset.mn-value.offset.sd, ymax=value.offset.mn+value.offset.sd,colour=aktSubstrate))
```

The first four time segments (1 - 4) show distinctly different responses. Beyond time segment 4 the responses between Akt and mTOR are less distinct.

### 3.4.3 Scatterplots
Use scatter plots to investigate if any Ins.1, LY, Ins.2, MK responses exhibit any noticeable difference between Akt and mTOR that may help differentiate between them.
```{r}
# features are not time based. Use master dataframe "dat"

cat.dat <- dat %>% mutate(Type = case_when(aktSubstrate==1~"Akt", mTORSubstrate == 1~"mTOR", TRUE~"Other"))  %>% select(Identifier, Seq.Window, Avg.Fold, AUC, Ins.1, LY, Ins.2, MK, aktSubstrate, mTORSubstrate, Type)

colnames(cat.dat)
head(cat.dat)
```

Use cat.dat to check the distributions of the categorical features.

#### Scatter plot of Ins.1, LY
Draw scatter plots of Ins.1 and LY to see if the features exhibit any noticeable difference between Akt and mTOR that may help differentiate between them.
```{r Check scatter plot of Ins.1, LY and Ins.2, MK}

sp <- ggplot(cat.dat) +
  geom_point(aes(x=Ins.1, y=LY, color=Type)) +
  xlab("Ins.1") +
  ylab("LY") +
  theme_light()

sp
```

#### Scatter plot of Ins.2, MK
Draw scatter plots of Ins.2 and MK to see if the features exhibit any noticeable difference between Akt and mTOR that may help differentiate between them.
```{r }
sp <- ggplot(cat.dat) +
  geom_point(aes(x=Ins.2, y=MK, color=Type)) +
  xlab("Ins.2") +
  ylab("MK") +
  theme_light()

sp
```

#### Scatter plot of Ins.2, Ins.1
```{r Check scatter plot of Ins.2, Ins.1}
sp <- ggplot(cat.dat) +
  geom_point(aes(x=Ins.2, y=Ins.1, color=Type)) +
  xlab("Ins.1") +
  ylab("Ins.2") +
  theme_light()

sp
```

#### Scatter plot of LY, MK
```{r Check scatter plot of LY, MK}
sp <- ggplot(cat.dat) +
  geom_point(aes(x=Ins.2, y=MK, color=Type)) +
  xlab("Ins.2") +
  ylab("MK") +
  theme_light()

sp
```

### 3.4.4 Boxplots and violin plots
#### Boxplots and violin plots of Avg.Fold by akt Substrate
```{r Check boxplot of Avg.Fold by akt Substrate, fig.height=7, fig.width=5}
require(gridExtra)

p1 <- ggplot(cat.dat) +
  geom_boxplot(aes(x=Type, y=Avg.Fold, color=Type)) +
  xlab("Type") +
  ylab("Avg.Fold") +
  ggtitle("Box Plot") + 
  theme_light()

p2 <- ggplot(cat.dat) +
  geom_violin(aes(x=Type, y=Avg.Fold, color=Type), scale = "area") +
  xlab("Type") +
  ylab("AUC") +
  ggtitle("Violin Plot") + 
  theme_light()

grid.arrange(p1, p2, ncol=1)
```

#### Boxplot and violin plot of AUC by akt Substrate
```{r Check boxplot of AUC by akt Substrate, fig.height=7, fig.width=5}
p1 <- ggplot(cat.dat) +
  geom_boxplot(aes(x=Type, y=AUC, color=Type)) +
  xlab("Type") +
  ylab("AUC") +
  ggtitle("Box Plot") + 
  theme_light()

p2 <- ggplot(cat.dat) +
  geom_violin(aes(x=Type, y=AUC, color=Type), scale = "area") +
  xlab("Type") +
  ylab("AUC") +
  ggtitle("Violin Plot") + 
  theme_light()

grid.arrange(p1, p2, ncol=1)
```

Akt and mTOR exhibit quite a significant difference in AUC, which is another statistic that comes from the shape of the Akt and mTOR response curves.

### 3.4.5 kernel density of Substrate
#### Plot the kernel density of AUC by substrate.
```{r Check kernel density of AUC by akt Substrate, another way to plot the distribution of AUC for each type}
p1 <- ggplot(cat.dat) +
  geom_density(aes(x=AUC, color=Type), n=1000, bw=0.025, trim=F) +
  scale_x_discrete(name="Type", limits=c(0,0.25,0.5,0.75,1.0,1.25)) +
  xlab("Type") +
  ylab("AUC") +
  ggtitle("Kernel Density of AUC") + 
  theme_light()

p1
```

#### Plot the kernel density of Avg.fold by substrate.
```{r Check kernel density of Avg.Fold by akt Substrate}
p1 <- ggplot(cat.dat) +
  geom_density(aes(x=Avg.Fold, color=Type), n=1000, trim=F) +
  
  xlab("Type") +
  ylab("Avg.Fold") +
  ggtitle("Kernel Density of Avg.Fold") + 
  theme_light()

p1
```

# 4. Classification Models
Four classification models were built:<br/>
1. Random Forest.<br/>
2. Support Vector Machine (SVM).<br/>
3. K-Nearest Neighbours (KNN).<br/>
4. Logistic Regression.<br/>

The purpose of building four models was to examine which model performed best. Two of the models are linear classifiers (SVM and Logistic Regression) and two models are non-linear (Random Forest and KNN). Feature creation was trialed and feature selection was also trialed with different models.

## 4.1 Random Forest model
### 4.1.1 Baseline Random Forest model
Create a baseline Random Forest model for prediction of Akt substrates and mTOR substrates
```{r Create folds}
# Create a subset dataframe that contains samples with known Akt or mTOR (i.e. exclude all unlabeled samples).
# Remove identifier, seq. window and mTOR substrate flag from subset.
known.dat <- dat[dat$aktSubstrate == 1 | dat$mTORSubstrate == 1, c(-1,-2,-18)]
# This subset dataframe is reasonably balanced in term of Akt (22) and mTOR (26) samples so there is no need for up or down sampling for the baseline.

# Determine the maximum number of predictors that can be used in the random forest
max.predictors <- length(colnames(known.dat)) - 1 # Substracting one as the last column is the response variable

set.seed(1) # Set seed to randomise the rows included in each fold
folds <- createFolds(known.dat[,"aktSubstrate"], 10) # Create 10 folds for cross validation

# Create a matrix to hold the results from the baseline random forest.
# Trail from 1 to 100 number of trees (number of rows = 100)
# Trail from 1 to max.predictors randomly sampled canditate variables for each split
gridSearchResults <- matrix(nrow=100, ncol=max.predictors)


for (m in 1:100){
  for(k in 1:max.predictors){
    
    fold.accuracy <- NULL
    
    for (tst in folds){
      
      known.dat.train = known.dat[-tst,]
      known.dat.test = known.dat[tst,]
      
      rf.Baseline <- randomForest(aktSubstrate~., data=known.dat.train,  mtry=k, replace=TRUE, importance=TRUE, ntree=m)
    
      ypred <-  predict(rf.Baseline, newdata=known.dat.test)
      ypred.accuracy <- sum(ypred == known.dat.test$aktSubstrate)/nrow(known.dat.test)
      
      if (is.null(fold.accuracy)){
        fold.accuracy <-  ypred.accuracy
        
      }
      else {
        fold.accuracy <- fold.accuracy + ypred.accuracy
        
      }
      
      #print(ypred.accuracy)
    }
    avg.accuracy <- fold.accuracy / length(folds)
    gridSearchResults[m, k] <-  avg.accuracy
    
  }
}
```

### 4.1.2 Summarise and visualise the random forest results.
Baseline random forest model predicts aktSubstrate from mTOR with accuracy of 81.5% (min) to 97.5% (max) using existing 14 features
```{r Plot results of rf.baseline}
summary(gridSearchResults) # Summaries the model results


p <- plot_ly(z = ~gridSearchResults) %>% add_surface()
p
```

### 4.1.3 Feature creation for Random Forest Model.
#### Sequence Window
Create new features for the Random Forest model to see if performance can be improved. The sequence window column contains the sequence of the phosphorylation site. It is a 13-amino acid sequence in which the 7th position corresponds to the amino acid that is phosphorylated. Breakout the sequence window into individual features.

```{r Breakout seq.window}
require(stringr)

cat.dat$Seq.Window <- as.character(cat.dat$Seq.Window)

str_length(cat.dat[1,]$Seq.Window)

# Create a new column for each of the each of the amino acids.
cat.dat$s1 <- as.factor(str_sub( cat.dat$Seq.Window, 1,1))
cat.dat$s2 <- as.factor(str_sub( cat.dat$Seq.Window, 2,2))
cat.dat$s3 <- as.factor(str_sub( cat.dat$Seq.Window, 3,3))
cat.dat$s4 <- as.factor(str_sub( cat.dat$Seq.Window, 4,4))
cat.dat$s5 <- as.factor(str_sub( cat.dat$Seq.Window, 5,5))
cat.dat$s6 <- as.factor(str_sub( cat.dat$Seq.Window, 6,6))
cat.dat$s7 <- as.factor(str_sub( cat.dat$Seq.Window, 7,7))
cat.dat$s8 <- as.factor(str_sub( cat.dat$Seq.Window, 8,8))
cat.dat$s9 <- as.factor(str_sub( cat.dat$Seq.Window, 9,9))
cat.dat$s10 <- as.factor(str_sub( cat.dat$Seq.Window, 10,10))
cat.dat$s11 <- as.factor(str_sub( cat.dat$Seq.Window, 11,11))
cat.dat$s12 <- as.factor(str_sub( cat.dat$Seq.Window, 12,12))
cat.dat$s13 <- as.factor(str_sub( cat.dat$Seq.Window, 13,13))
```

#### Visualise each seq character in the seq window. 
Generate plots for each seq character in the seq window position from position 1 to 13
```{r Generate plots for each seq character in the seq window position from position 1 to 13}
for (seqName in colnames(cat.dat)[12:24]){
  myplot <- ggplot(cat.dat, aes_string(x=seqName, group = "Type")) +
            geom_bar(aes(y = ..prop.., fill = Type), stat="count") +
            scale_y_continuous(labels=scales::percent) +
            ylab("relative frequencies") +
            ggtitle(seqName)+
            facet_grid(~Type)


  # myplot

  plotFilename <- paste("./Plots/seq_",seqName,".png",sep="_")
  ggsave(plotFilename, myplot, dpi=320, device="png")

  print(paste("saved plot to: ",plotFilename," ...",sep=""))

}
```

Some sequence character character positions show patterns with their amino acid profile (e.g. 45% of S1 for Akt are F)

#### Rate of Change Feature
The visual exploration of the response data showed that Akt and mTOR have distinct profiles in the first four time segments. Features have been created to take advantage of this distinguishing attribute, and hopefully improve model performance.
```{r Create some additional features based on exploratory data analysis above.}
dat.extra <- dat %>% mutate(diffP1 = X30s - X15s,
                            diffP2 = X1m - X30s,
                            diffP3 = X2m - X1m,
                            diffP4 = X5m - X2m,
                            diffP5 = X10m - X5m,
                            diffP6 = X20m - X10m,
                            diffP7 = X60m - X20m,
                            Type = as.factor(case_when(aktSubstrate==1~"Akt", mTORSubstrate == 1~"mTOR", TRUE~"Other")),
                            s1 = as.factor(str_sub( Seq.Window, 1,1)),
                            s2 = as.factor(str_sub( Seq.Window, 2,2)),
                            s3 = as.factor(str_sub( Seq.Window, 3,3)),
                            s4 = as.factor(str_sub( Seq.Window, 4,4)),
                            s5 = as.factor(str_sub( Seq.Window, 5,5)),
                            s6 = as.factor(str_sub( Seq.Window, 6,6)),
                            s7 = as.factor(str_sub( Seq.Window, 7,7)),
                            s8 = as.factor(str_sub( Seq.Window, 8,8)),
                            s9 = as.factor(str_sub( Seq.Window, 9,9)),
                            s10 = as.factor(str_sub( Seq.Window, 10,10)),
                            s11 = as.factor(str_sub( Seq.Window, 11,11)),
                            s12 = as.factor(str_sub( Seq.Window, 12,12)),
                            s13 = as.factor(str_sub( Seq.Window, 13,13))
                            )


```

### 4.1.4 Create Random Forest model using new features
Test the influence of the new features on the accuracy of the random forest model by comparing to the baseline accuracy.
```{r Remove all unlabeled samples for this test to see whether the additional features are useful for model performance}
# Use identical procedure as the baseline so the influence of the new feasures can be compared to the baseline
dat.extra.rf <- dat.extra %>% filter(Type != "Other") %>% select(-Identifier, -Seq.Window, -mTORSubstrate, -Type)
folds <- createFolds(dat.extra.rf$aktSubstrate, 10)


```

#### Grid search to find best parameters
```{r Execute a grid search between 1 to 100 trees, and 1 to 34 candidate predictors available for each split using 10 fold CV }
max.predictors.dat.extra <- length(colnames(dat.extra.rf))-1

gridSearchResults.extra <- matrix(nrow=100, ncol=max.predictors.dat.extra)


for (m in 1:100){
  for(k in 1:max.predictors.dat.extra){

  fold.accuracy <- NULL
  
  for (tst in folds){
    
    dat.extra.rf.train = dat.extra.rf[-tst,]
    dat.extra.rf.test = dat.extra.rf[tst,]
    
    rf.ExtraFeatures <- randomForest(aktSubstrate~., data=dat.extra.rf.train,  mtry=k, replace=TRUE, importance=TRUE, ntree=m) # importance based on 
  
    ypred.prob <-  predict(rf.ExtraFeatures, newdata=dat.extra.rf.test, type="prob")
    ypred <- ifelse(ypred.prob[,2] > 0.5, 1, 0)
    ypred.accuracy <- sum(ypred == dat.extra.rf.test$aktSubstrate)/nrow(dat.extra.rf.test)
    
    if (is.null(fold.accuracy)){
      fold.accuracy <-  ypred.accuracy
      
    }
    else {
      fold.accuracy <- fold.accuracy + ypred.accuracy
      
    }
  
    }
    avg.accuracy <- fold.accuracy / length(folds)

    avg.accuracy    

    gridSearchResults.extra[m, k] <-  avg.accuracy
    
  }
}

# View(gridSearchResults.extra)
write.csv(gridSearchResults.extra, './Output/gridSearchResults.csv')
# 5 trees and 4 predictors is all that's needed


```

#### Visualise grid search results of revised random forest model
```{r 3D Plot results of rf.extra model, and find the best model based on accuracy, noting that number of class samples in this scenario are not too imbalanced.}
require(plotly)

p.extra <- plot_ly(z = ~gridSearchResults.extra) %>% add_surface()
p.extra

rf.ExtraFeatures$importance

max(gridSearchResults.extra)
```

### 4.1.5 Summary of Random Forest.
Below is the summary of Random forest model and proposal for next steps.<br/>
a) The average out of bag (OOB) estimate of error rate decreases from 9.09% to 6.98% with the new features added..<br/>
b) Some mtry and number of tree configurations achieve 100% accuracy..<br/>
c) Apply adaptive sampling procedure to the dataset in order to classify the unlabeled data..<br/>


## 4.2 SVM model
For the SVM model no features selection or creation will be used.

#### SVM model - The aktSubstrate as class
Prepare data for SVM model 
```{r}
names(dat)[c(1,2,18)] 
# create an index to remember which rows are positively labelled as akt
aktIdx = which(dat$aktSubstrate==1)  
# create an index to remember which rows are positively labelled as mTOR
mTORIdx = which(dat$mTORSubstrate==1) 

aktSubstrate.dat <- dat[c(-1,-2,-18)]
# keep a copy for comparison later since we overwrite this feature in the line below
aktSubstrate.dat$aktSubstrate_Original = aktSubstrate.dat$aktSubstrate 
# this does not make "0" into 0, and "1" into 1
aktSubstrate.dat$aktSubstrate = as.numeric(as.factor(aktSubstrate.dat$aktSubstrate)) 

aktSubstrate.dat$aktSubstrate_corrected = as.numeric(as.factor(aktSubstrate.dat$aktSubstrate)) - 1

dfCompare <- data.frame(aktSubstrate.dat$aktSubstrate_Original, aktSubstrate.dat$aktSubstrate, aktSubstrate.dat$aktSubstrate_corrected)

names(dfCompare) <- c("Original", "Factor to Numeric Conversion", "Corrected")

# let's take a look at the original field, the numeric conversion, and the corrected numerical conversion.
# Note: The column types are listed under the column names, [factor, double, double]
head(dfCompare)

dim(aktSubstrate.dat)
```
#### Build SVM model and calculate performance metrics 
```{r}
aktSubstrate_data.mat <- apply(X = aktSubstrate.dat[,-(15:17)], MARGIN = 2, FUN =  as.numeric) 

aktSubstrate_data.mat <- aktSubstrate_data.mat[c(aktIdx,mTORIdx),]#TKP Need to remove the unlabeled items, otherwise the class imbalance is too large for the SVM to train.

aktSubstrate_known.dat<- aktSubstrate_data.mat

attr(aktSubstrate_data.mat, "dimnames")[2]

data.cls.truth <- sapply(X = aktSubstrate.dat$aktSubstrate_corrected, FUN = function(x) {ifelse(x=="1", 1, 0)}) # try with the corrected truth class instead

data.cls.truth_akt <- data.cls.truth[c(aktIdx,mTORIdx)] # Need to match the indexed rows

rownames(aktSubstrate_data.mat) <- paste("p", 1:nrow(aktSubstrate_data.mat), sep="_")

#data.cls.truth


k <- 10
set.seed(1)
fold <- createFolds(data.cls.truth_akt, 10);
# gold standard (orignal data)
TP <- TN <- FP <- FN <- c()
for(i in 1:length(fold)){

    model <- svm(aktSubstrate_data.mat[-fold[[i]],], data.cls.truth_akt[-fold[[i]]])
    preds <- ifelse(predict(model, aktSubstrate_data.mat[fold[[i]],]) > 0.5, 1, 0)
    TP <- c(TP, sum((data.cls.truth_akt[fold[[i]]] == preds)[data.cls.truth_akt[fold[[i]]] == "1"]))
    TN <- c(TN, sum((data.cls.truth_akt[fold[[i]]] == preds)[data.cls.truth_akt[fold[[i]]] == "0"]))
    FP <- c(FP, sum((data.cls.truth_akt[fold[[i]]] != preds)[preds == "1"]))
    FN <- c(FN, sum((data.cls.truth_akt[fold[[i]]] != preds)[preds == "0"]))
    
}
mean(F1(cbind(TN, FP, TP, FN)))
mean(Sen(cbind(TN, FP, TP, FN)))
mean(Spe(cbind(TN, FP, TP, FN)))
mean(Acc(cbind(TN, FP, TP, FN)))
```
#### SVM model - The mTORSubstrate as class
Prepare data for SVM model 
```{r}
mTORSubstrate.dat <- dat[c(-1,-2,-17)]
# keep a copy for comparison later since we overwrite this feature in the line below
mTORSubstrate.dat$mTORSubstrate_Original = mTORSubstrate.dat$mTORSubstrate 
# this does not make "0" into 0, and "1" into 1
mTORSubstrate.dat$mTORSubstrate = as.numeric(as.factor(mTORSubstrate.dat$mTORSubstrate)) 

mTORSubstrate.dat$mTORSubstrate_corrected = as.numeric(as.factor(mTORSubstrate.dat$mTORSubstrate)) - 1

dfCompare <- data.frame(mTORSubstrate.dat$mTORSubstrate_Original, mTORSubstrate.dat$mTORSubstrate, mTORSubstrate.dat$mTORSubstrate_corrected)

names(dfCompare) <- c("Original", "Factor to Numeric Conversion", "Corrected")

# let's take a look at the original field, the numeric conversion, and the corrected numerical conversion.
# Note: The column types are listed under the column names, [factor, double, double]
head(dfCompare)

dim(mTORSubstrate.dat)
```
#### Build SVM model and calculate performance metrics 
```{r}
mTORSubstrate_data.mat <- apply(X = mTORSubstrate.dat[,-(15:17)], MARGIN = 2, FUN =  as.numeric) #TKP slightly modified to also drop cols 16 and 17 since I added two additional columns to create dfCompare above.

mTORSubstrate_data.mat <- mTORSubstrate_data.mat[c(aktIdx,mTORIdx),]#TKP Need to remove the unlabeled items, otherwise the class imbalance is too large for the SVM to train.

mTORSubstrate_known.dat<- mTORSubstrate_data.mat

attr(mTORSubstrate_data.mat, "dimnames")[2]

data.cls.truth_mTOR <- sapply(X = mTORSubstrate.dat$mTORSubstrate_corrected, FUN = function(x) {ifelse(x=="1", 1, 0)}) #TKP try with the corrected truth class instead

data.cls.truth_mTOR <- data.cls.truth_mTOR[c(aktIdx,mTORIdx)] # Need to match the indexed rows

rownames(mTORSubstrate_data.mat) <- paste("p", 1:nrow(mTORSubstrate_data.mat), sep="_")

#data.cls.truth

k <- 10
set.seed(1)
fold <- createFolds(data.cls.truth_mTOR, 10);
# gold standard (orignal data)
TP <- TN <- FP <- FN <- c()
for(i in 1:length(fold)){
    model <- svm(mTORSubstrate_data.mat[-fold[[i]],], data.cls.truth_mTOR[-fold[[i]]])
    preds <- ifelse(predict(model, mTORSubstrate_data.mat[fold[[i]],]) > 0.5, 1, 0)
    TP <- c(TP, sum((data.cls.truth_mTOR[fold[[i]]] == preds)[data.cls.truth_mTOR[fold[[i]]] == "1"]))
    TN <- c(TN, sum((data.cls.truth_mTOR[fold[[i]]] == preds)[data.cls.truth_mTOR[fold[[i]]] == "0"]))
    FP <- c(FP, sum((data.cls.truth_mTOR[fold[[i]]] != preds)[preds == "1"]))
    FN <- c(FN, sum((data.cls.truth_mTOR[fold[[i]]] != preds)[preds == "0"]))
}
mean(F1(cbind(TN, FP, TP, FN)))
mean(Sen(cbind(TN, FP, TP, FN)))
mean(Spe(cbind(TN, FP, TP, FN)))
mean(Acc(cbind(TN, FP, TP, FN)))
```

## 4.3 KNN Model
KNN model was built using wrapper method for feature selection to see if feature selection could remove noise from the data and improve model performance.<br/>
#### KNN Model for aktSubstrate as class:
```{r}
selectFeature <- function(train, test, cls.train, cls.test, features) {
  ## identify a feature to be selected
  current.best.accuracy <- -Inf
  selected.i <- NULL
  for(i in 1:ncol(train)) {
    current.f <- colnames(train)[i]
    if(!current.f %in% features) {
      model <- knn(train=train[,c(features, current.f)], test=test[,c(features, current.f)], cl=cls.train, k=9)
      test.acc <- sum(model == cls.test) / length(cls.test)
      
      if(test.acc > current.best.accuracy) {
        current.best.accuracy <- test.acc
        selected.i <- colnames(train)[i]
      }
    }
  }
  return(selected.i)
}



set.seed(1)
inTrain <- createDataPartition(data.cls.truth_akt, p = .6)[[1]]
allFeatures <- colnames(aktSubstrate_data.mat)
train <- aktSubstrate_data.mat[ inTrain,]
test  <- aktSubstrate_data.mat[-inTrain,]
cls.train <- data.cls.truth_akt[inTrain]
cls.test <- data.cls.truth_akt[-inTrain]

# use correlation to determine the first feature
cls.train.numeric <- rep(c(0, 1), c(sum(cls.train == "0"), sum(cls.train == "1")))
features <- c()
current.best.cor <- 0
for(i in 1:ncol(train)) {
  if(current.best.cor < abs(cor(train[,i], cls.train.numeric))) {
    current.best.cor <- abs(cor(train[,i], cls.train.numeric))
    features <- colnames(train)[i]
  }
}
print(features)

for (j in 2:10) {
  selected.i <- selectFeature(train, test, cls.train, cls.test, features)
  print(selected.i)

  # add the best feature from current run
  features <- c(features, selected.i)
}

features_selection_aktSubstrate_data.mat <- aktSubstrate_data.mat[,features]

k <- 10
set.seed(1)
fold <- createFolds(data.cls.truth_akt, 10);
# gold standard (orignal data)
TP <- TN <- FP <- FN <- c()
for(i in 1:length(fold)){
  
    #model <- svm(features_selection_aktSubstrate_data.mat[-fold[[i]],], data.cls.truth_akt[-fold[[i]]])
  preds <- knn (train=features_selection_aktSubstrate_data.mat[-fold[[i]],], test=features_selection_aktSubstrate_data.mat[fold[[i]],], cl=data.cls.truth_akt[-fold[[i]]], k=9)
  
    #preds <- ifelse(predict(model, features_selection_aktSubstrate_data.mat[fold[[i]],]) > 0.5, 1, 0)
    TP <- c(TP, sum((data.cls.truth_akt[fold[[i]]] == preds)[data.cls.truth_akt[fold[[i]]] == "1"]))
    TN <- c(TN, sum((data.cls.truth_akt[fold[[i]]] == preds)[data.cls.truth_akt[fold[[i]]] == "0"]))
    FP <- c(FP, sum((data.cls.truth_akt[fold[[i]]] != preds)[preds == "1"]))
    FN <- c(FN, sum((data.cls.truth_akt[fold[[i]]] != preds)[preds == "0"]))
    
}
mean(F1(cbind(TN, FP, TP, FN)))
mean(Sen(cbind(TN, FP, TP, FN)))
mean(Spe(cbind(TN, FP, TP, FN)))
mean(Acc(cbind(TN, FP, TP, FN)))
```

#### KNN Model for mTORSubstrate as class:
```{r}
set.seed(1)
inTrain <- createDataPartition(data.cls.truth_mTOR, p = .6)[[1]]
allFeatures <- colnames(mTORSubstrate_data.mat)
train <- mTORSubstrate_data.mat[ inTrain,]
test  <- mTORSubstrate_data.mat[-inTrain,]
cls.train <- data.cls.truth_mTOR[inTrain]
cls.test <- data.cls.truth_mTOR[-inTrain]

# use correlation to determine the first feature
cls.train.numeric <- rep(c(0, 1), c(sum(cls.train == "0"), sum(cls.train == "1")))
features <- c()
current.best.cor <- 0
for(i in 1:ncol(train)) {
  if(current.best.cor < abs(cor(train[,i], cls.train.numeric))) {
    current.best.cor <- abs(cor(train[,i], cls.train.numeric))
    features <- colnames(train)[i]
  }
}
print(features)

for (j in 2:10) {
  selected.i <- selectFeature(train, test, cls.train, cls.test, features)
  print(selected.i)

  # add the best feature from current run
  features <- c(features, selected.i)
}

features_selection_mTORSubstrate_data.mat <- mTORSubstrate_data.mat[,features]

k <- 10
set.seed(1)
fold <- createFolds(data.cls.truth_mTOR, 10);
# gold standard (orignal data)
TP <- TN <- FP <- FN <- c()
for(i in 1:length(fold)){
  
    #model <- svm(features_selection_mTORSubstrate_data.mat[-fold[[i]],], data.cls.truth_mTOR[-fold[[i]]])
    preds <- knn (train=features_selection_mTORSubstrate_data.mat[-fold[[i]],], test=features_selection_mTORSubstrate_data.mat[fold[[i]],], cl=data.cls.truth_mTOR[-fold[[i]]], k=9)
      
    #preds <- ifelse(predict(model, features_selection_mTORSubstrate_data.mat[fold[[i]],]) > 0.5, 1, 0)
    TP <- c(TP, sum((data.cls.truth_mTOR[fold[[i]]] == preds)[data.cls.truth_mTOR[fold[[i]]] == "1"]))
    TN <- c(TN, sum((data.cls.truth_mTOR[fold[[i]]] == preds)[data.cls.truth_mTOR[fold[[i]]] == "0"]))
    FP <- c(FP, sum((data.cls.truth_mTOR[fold[[i]]] != preds)[preds == "1"]))
    FN <- c(FN, sum((data.cls.truth_mTOR[fold[[i]]] != preds)[preds == "0"]))
    
}
mean(F1(cbind(TN, FP, TP, FN)))
mean(Sen(cbind(TN, FP, TP, FN)))
mean(Spe(cbind(TN, FP, TP, FN)))
mean(Acc(cbind(TN, FP, TP, FN)))
```
Feature selection did not cause the KNN model to perform at higher accuracy based on known samples.

## 4.4 Logistic regression Model.
The logistic regression model was build using step-wise feature selection to see if feature selection could remove noise from the data and improve model performance.<br/>

### 4.4.1 Baseline Logistic regression model for aktSubstrate class
```{r Baseline Logistic regression model}
glm.akt<-glm(dat$aktSubstrate~dat$Avg.Fold+dat$AUC+dat$X15s+dat$X30s+dat$X1m+dat$X2m+dat$X5m+dat$X10m+dat$X20m+dat$X60m+dat$Ins.1+dat$LY+dat$Ins.2+dat$MK,data=dat,family = binomial)
summary(glm.akt)
```

### 4.4.2 Feature selection using stepwise for aktSubstrate class
```{r stepwise feature selection}
#feature selection to build the model step by step
glm.akt1<-step(glm.akt)
summary(glm.akt1)

#explain the model
coef(glm.akt1)
exp(glm.akt1$coefficients)#the relationship between odds and x
exp(confint(glm.akt1))#Confidence interval
xp0.5<-0/glm.akt1$coefficients[]
xp0.5#return x which makes pi equal to 0.5
ratio05<-glm.akt1$coefficients[]*0.25
ratio05

```
### 4.4.3 Evaluate Logistic Regression baseline model for aktSubstrate class
```{r}
#evaluate the model
R2cox<-1-exp((glm.akt1$deviance-glm.akt1$null.deviance)/length(dat$aktSubstrate))
cat("Cox-Snell R2=",R2cox,"\n")
R2nag<-R2cox/(1-exp((-glm.akt1$null.deviance)/length(dat$aktSubstrate)))
R2nag
cat("Nagelkerke R2=",R2nag,"\n")
plot(residuals(glm.akt1))
#install.packages('car')

influencePlot(glm.akt1)
```

#### Summary of evaluation:
```{r}
fitted.pi<-fitted(glm.akt1)
ypred<-1*(fitted.pi>0.5)
length(ypred)
n<-table(dat$aktSubstrate,ypred)
n
cat("sensitivity=",n[2,2]/sum(n[2,]),"\n")
cat("specificity=",n[1,1]/sum(n[1,]),"\n")
```


### 4.4.4 Baseline Logistic Regression model for mTORSubstrate class
```{r Baseline Logistic regression model for mTORSubstrate class}
#create the logistic regression taking all the features
glm.mtor<-glm(dat$mTORSubstrate~dat$Avg.Fold+dat$AUC+dat$X15s+dat$X30s+dat$X1m+dat$X2m+dat$X5m+dat$X10m+dat$X20m+dat$X60m+dat$Ins.1+dat$LY+dat$Ins.2+dat$MK,data=dat,family = binomial)
summary(glm.mtor)
```

### 4.4.5 Feature selection using stepwise for mTORSubstrate class
```{r}
#feature selection to build the model step by step
glm.mtor1<-step(glm.mtor)
summary(glm.mtor1)

#explain the model
coef(glm.mtor1)
exp(glm.mtor1$coefficients)#the relationship between odds and x
exp(confint(glm.mtor1))#Confidence interval
xp0.5<-0/glm.mtor1$coefficients[]
xp0.5#return x which makes pi equal to 0.5
ratio05<-glm.mtor1$coefficients[]*0.25
ratio05

```
### 4.4.6 Evaluate Logistic Regression baseline model for mTORSubstrate class
```{r}
#evaluate the model
R2cox<-1-exp((glm.mtor1$deviance-glm.mtor1$null.deviance)/length(dat$mTORSubstrate))
cat("Cox-Snell R2=",R2cox,"\n")
R2nag<-R2cox/(1-exp((-glm.mtor1$null.deviance)/length(dat$mTORSubstrate)))
R2nag
cat("Nagelkerke R2=",R2nag,"\n")
plot(residuals(glm.mtor1))
influencePlot(glm.mtor1)
```

#### Summary of evaluation:
```{r}
fitted.pi<-fitted(glm.mtor1)
ypred<-1*(fitted.pi>0.5)
length(ypred)
n<-table(dat$mTORSubstrate,ypred)
n
cat("sensitivity=",n[2,2]/sum(n[2,]),"\n")
cat("specificity=",n[1,1]/sum(n[1,]),"\n")
```


# 5. Adaptive Sampling
Adaptive sampling has been applied to the four models. Adaptive sampling assigns probability to the unlabeled data of being positive (e.g. Akt) or negative (e.g. not Akt). Once the class probability of the unlabeled data is assigned it is then fed back into the model on choice to give the model additional data, ideally resulting in better model performance.

## 5.1 AdaSampling with SVM

#### AktSubstrate class
```{r}
# create empty list to combine all results for analysis.
result.final <- c()

dat$Identifier <- NULL
dat$Seq.Window <-NULL

# create an index to remember which rows are positively labelled as akt
aktIdx = which(dat$aktSubstrate==1) 
# create an index to remember which rows are positively labelled as mTOR
mTORIdx = which(dat$mTORSubstrate==1) 

aktSubstrate.dat <- dat[c(-16)]

# this does not make "0" into 0, and "1" into 1
aktSubstrate.dat$aktSubstrate = as.numeric(as.factor(aktSubstrate.dat$aktSubstrate)) 

aktSubstrate.dat$aktSubstrate_corrected = as.numeric(as.factor(aktSubstrate.dat$aktSubstrate)) - 1

dim(aktSubstrate.dat)
```

```{r}



data.cls.truth <- aktSubstrate.dat$aktSubstrate_corrected

aktSubstrate.dat <- aktSubstrate.dat[,-c(15:16)]

dim(aktSubstrate.dat)
```

```{r}
k = 10
set.seed(1)
fold=createFolds(data.cls.truth, k)
TP <- TN <- FP <- FN <- c()
for (i in 1:length(fold)){
  train.mat = aktSubstrate.dat[-fold[[i]],]
  test.mat = aktSubstrate.dat[fold[[i]],]
  cls = data.cls.truth[-fold[[i]]]
  
  #index positive and genative instances
  Ps = rownames(train.mat)[which(cls == 1)]
  Ns = rownames(train.mat)[which(cls == 0)]
  
  pred.prob = adaSample_SVM(Ps, Ns, train.mat, test.mat, classifier = "svm", cost=1, gamma=0.01)
  pred = ifelse(pred.prob[, "P"] >0.5, 1, 0)
  
  TP <- c(TP, sum((data.cls.truth[fold[[i]]] == pred)[data.cls.truth[fold[[i]]] == "1"]))
  TN <- c(TN, sum((data.cls.truth[fold[[i]]] == pred)[data.cls.truth[fold[[i]]] == "0"]))
  FP <- c(FP, sum((data.cls.truth[fold[[i]]] != pred)[pred == "1"]))
  FN <- c(FN, sum((data.cls.truth[fold[[i]]] != pred)[pred == "0"]))
}

mean(F1(cbind(TN, FP, TP, FN)))
mean(Sen(cbind(TN, FP, TP, FN)))
mean(Spe(cbind(TN, FP, TP, FN)))
mean(Acc(cbind(TN, FP, TP, FN)))

result.final <- rbind(result.final, c("AdaSampling (svm)", "Akt", 
                  mean(F1(cbind(TN, FP, TP, FN))),
                  mean(Acc(cbind(TN, FP, TP, FN))),
                  mean(Sen(cbind(TN, FP, TP, FN))),
                  mean(Spe(cbind(TN, FP, TP, FN)))))


posLikeRatio = mean(Sen(cbind(TN, FP, TP, FN)))/(1-mean(Spe(cbind(TN, FP, TP, FN))))
paste("AdaSampling (svm) for.Akt - Positive Likelihood Ratio = ",posLikeRatio)

```

#### mTORSubstrate class
```{r}

mTORSubstrate.dat <- dat[c(-15)]

mTORSubstrate.dat$mTORSubstrate = as.numeric(as.factor(mTORSubstrate.dat$mTORSubstrate)) #TKP this does not make "0" into 0, and "1" into 1


## TKP code ##
mTORSubstrate.dat$mTORSubstrate_corrected = as.numeric(as.factor(mTORSubstrate.dat$mTORSubstrate)) - 1

dim(mTORSubstrate.dat)
```
```{r}

data.cls.truth <- mTORSubstrate.dat$mTORSubstrate_corrected

mTORSubstrate.dat <- mTORSubstrate.dat[,-c(15:16)]

```

```{r}
k = 10
set.seed(1)

#data.cls.truth

fold=createFolds(data.cls.truth, k)
TP <- TN <- FP <- FN <- c()
for (i in 1:length(fold)){
  train.mat = mTORSubstrate.dat[-fold[[i]],]
  test.mat = mTORSubstrate.dat[fold[[i]],]
  cls = data.cls.truth[-fold[[i]]]
  
  #index positive and genative instances
  Ps = rownames(train.mat)[which(cls == 1)]
  Ns = rownames(train.mat)[which(cls == 0)]
  
  pred.prob = adaSample_SVM(Ps, Ns, train.mat, test.mat, classifier = "svm", cost=1, gamma=0.01)
  pred = ifelse(pred.prob[, "P"] >0.5, 1, 0)
  
  TP <- c(TP, sum((data.cls.truth[fold[[i]]] == pred)[data.cls.truth[fold[[i]]] == "1"]))
  TN <- c(TN, sum((data.cls.truth[fold[[i]]] == pred)[data.cls.truth[fold[[i]]] == "0"]))
  FP <- c(FP, sum((data.cls.truth[fold[[i]]] != pred)[pred == "1"]))
  FN <- c(FN, sum((data.cls.truth[fold[[i]]] != pred)[pred == "0"]))
}

mean(F1(cbind(TN, FP, TP, FN)))
mean(Sen(cbind(TN, FP, TP, FN)))
mean(Spe(cbind(TN, FP, TP, FN)))
mean(Acc(cbind(TN, FP, TP, FN)))

# add to result.final
result.final <- rbind(result.final,c("AdaSampling (svm)", "mToR", 
                  mean(F1(cbind(TN, FP, TP, FN))),
                  mean(Acc(cbind(TN, FP, TP, FN))),
                  mean(Sen(cbind(TN, FP, TP, FN))),
                  mean(Spe(cbind(TN, FP, TP, FN)))))

posLikeRatio = mean(Sen(cbind(TN, FP, TP, FN)))/(1-mean(Spe(cbind(TN, FP, TP, FN))))
paste("AdaSampling (svm) for mToR - Positive Likelihood Ratio = ",posLikeRatio)
```

## 5.2 Ada Sampling using KNN
#### AktSubstrate class
```{r Ada Sampling using KNN for aktSubstrate class}

dat$Identifier <- NULL
dat$Seq.Window <-NULL

# create an index to remember which rows are positively labelled as akt
aktIdx = which(dat$aktSubstrate==1) 
# create an index to remember which rows are positively labelled as mTOR
mTORIdx = which(dat$mTORSubstrate==1) 

aktSubstrate.dat <- dat[c(-16)]

```

```{r}
# this does not make "0" into 0, and "1" into 1
aktSubstrate.dat$aktSubstrate = as.numeric(as.factor(aktSubstrate.dat$aktSubstrate)) 

aktSubstrate.dat$aktSubstrate_corrected = as.numeric(as.factor(aktSubstrate.dat$aktSubstrate)) - 1


dim(aktSubstrate.dat)
aktSubstrate.dat[1:2,]
data.cls.truth <- aktSubstrate.dat$aktSubstrate_corrected

aktSubstrate.dat <- aktSubstrate.dat[,-c(15:16)]

```

```{r}


k = 10
set.seed(1)
fold=createFolds(data.cls.truth, k)
TP <- TN <- FP <- FN <- c()
for (i in 1:length(fold)){
  train.mat = aktSubstrate.dat[-fold[[i]],]
  test.mat = aktSubstrate.dat[fold[[i]],]
  cls = data.cls.truth[-fold[[i]]]
  
  #index positive and genative instances
  Ps = rownames(train.mat)[which(cls == 1)]
  Ns = rownames(train.mat)[which(cls == 0)]
  
  pred.prob = adaSample(Ps, Ns, train.mat, test.mat, classifier = "knn")
  pred = ifelse(pred.prob[, "P"] >0.5, 1, 0)
  
  TP <- c(TP, sum((data.cls.truth[fold[[i]]] == pred)[data.cls.truth[fold[[i]]] == "1"]))
  TN <- c(TN, sum((data.cls.truth[fold[[i]]] == pred)[data.cls.truth[fold[[i]]] == "0"]))
  FP <- c(FP, sum((data.cls.truth[fold[[i]]] != pred)[pred == "1"]))
  FN <- c(FN, sum((data.cls.truth[fold[[i]]] != pred)[pred == "0"]))
}

mean(F1(cbind(TN, FP, TP, FN)))
mean(Sen(cbind(TN, FP, TP, FN)))
mean(Spe(cbind(TN, FP, TP, FN)))
mean(Acc(cbind(TN, FP, TP, FN)))

result.final <- rbind(result.final, c("AdaSampling (knn)", "Akt", 
                  mean(F1(cbind(TN, FP, TP, FN))),
                  mean(Acc(cbind(TN, FP, TP, FN))),
                  mean(Sen(cbind(TN, FP, TP, FN))),
                  mean(Spe(cbind(TN, FP, TP, FN)))))


posLikeRatio = mean(Sen(cbind(TN, FP, TP, FN)))/(1-mean(Spe(cbind(TN, FP, TP, FN))))
paste("AdaSampling (knn) for.Akt - Positive Likelihood Ratio = ",posLikeRatio)

```

#### mTORSubstrate class
```{r Ada Sampling using KNN for mTORSubstrate class}

mTORSubstrate.dat <- dat[c(-15)]

mTORSubstrate.dat$mTORSubstrate = as.numeric(as.factor(mTORSubstrate.dat$mTORSubstrate)) #TKP this does not make "0" into 0, and "1" into 1


## TKP code ##
mTORSubstrate.dat$mTORSubstrate_corrected = as.numeric(as.factor(mTORSubstrate.dat$mTORSubstrate)) - 1

## TKP code end ##

data.cls.truth <- mTORSubstrate.dat$mTORSubstrate_corrected
mTORSubstrate.dat <- mTORSubstrate.dat[,-c(15:16)]

dim(mTORSubstrate.dat)
```

```{r}

k = 10
set.seed(1)
fold=createFolds(data.cls.truth, k)
TP <- TN <- FP <- FN <- c()
for (i in 1:length(fold)){
  train.mat = mTORSubstrate.dat[-fold[[i]],]
  test.mat = mTORSubstrate.dat[fold[[i]],]
  cls = data.cls.truth[-fold[[i]]]
  
  #index positive and genative instances
  Ps = rownames(train.mat)[which(cls == 1)]
  Ns = rownames(train.mat)[which(cls == 0)]
  
  pred.prob = adaSample(Ps, Ns, train.mat, test.mat, classifier = "knn")
  pred = ifelse(pred.prob[, "P"] >0.5, 1, 0)
  
  TP <- c(TP, sum((data.cls.truth[fold[[i]]] == pred)[data.cls.truth[fold[[i]]] == "1"]))
  TN <- c(TN, sum((data.cls.truth[fold[[i]]] == pred)[data.cls.truth[fold[[i]]] == "0"]))
  FP <- c(FP, sum((data.cls.truth[fold[[i]]] != pred)[pred == "1"]))
  FN <- c(FN, sum((data.cls.truth[fold[[i]]] != pred)[pred == "0"]))
}

mean(F1(cbind(TN, FP, TP, FN)))
mean(Sen(cbind(TN, FP, TP, FN)))
mean(Spe(cbind(TN, FP, TP, FN)))
mean(Acc(cbind(TN, FP, TP, FN)))

# add to result.final
result.final <- rbind(result.final,c("AdaSampling (knn)", "mToR", 
                  mean(F1(cbind(TN, FP, TP, FN))),
                  mean(Acc(cbind(TN, FP, TP, FN))),
                  mean(Sen(cbind(TN, FP, TP, FN))),
                  mean(Spe(cbind(TN, FP, TP, FN)))))

posLikeRatio = mean(Sen(cbind(TN, FP, TP, FN)))/(1-mean(Spe(cbind(TN, FP, TP, FN))))
paste("AdaSampling (knn) for mToR - Positive Likelihood Ratio = ",posLikeRatio)


```

## 5.3 Ada Sampling using Logistic Regression model
#### AktSubstrate class
```{r Ada Sampling using logit for aktSubstrate class}
# create an index to remember which rows are positively labelled as akt
aktIdx = which(dat$aktSubstrate==1) 
# create an index to remember which rows are positively labelled as mTOR
mTORIdx = which(dat$mTORSubstrate==1) 

aktSubstrate.dat <- dat[c(-16)]

# this does not make "0" into 0, and "1" into 1
aktSubstrate.dat$aktSubstrate = as.numeric(as.factor(aktSubstrate.dat$aktSubstrate)) 

aktSubstrate.dat$aktSubstrate_corrected = as.numeric(as.factor(aktSubstrate.dat$aktSubstrate)) - 1


dim(aktSubstrate.dat)
aktSubstrate.dat[1:2,]
data.cls.truth <- aktSubstrate.dat$aktSubstrate_corrected
aktSubstrate.dat <- aktSubstrate.dat[,-c(15:16)]

```

```{r}
k = 10
set.seed(1)
fold=createFolds(data.cls.truth, k)
TP <- TN <- FP <- FN <- c()
for (i in 1:length(fold)){
  train.mat = aktSubstrate.dat[-fold[[i]],-13]
  test.mat = aktSubstrate.dat[fold[[i]],-13]
  cls = data.cls.truth[-fold[[i]]]
  
  #index positive and genative instances
  Ps = rownames(train.mat)[which(cls == 1)]
  Ns = rownames(train.mat)[which(cls == 0)]
  
  pred.prob = adaSample(Ps, Ns, train.mat, test.mat, classifier = "logit")
  pred = ifelse(pred.prob[, "P"] >0.5, 1, 0)
  
  TP <- c(TP, sum((data.cls.truth[fold[[i]]] == pred)[data.cls.truth[fold[[i]]] == "1"]))
  TN <- c(TN, sum((data.cls.truth[fold[[i]]] == pred)[data.cls.truth[fold[[i]]] == "0"]))
  FP <- c(FP, sum((data.cls.truth[fold[[i]]] != pred)[pred == "1"]))
  FN <- c(FN, sum((data.cls.truth[fold[[i]]] != pred)[pred == "0"]))
}

mean(F1(cbind(TN, FP, TP, FN)))
mean(Sen(cbind(TN, FP, TP, FN)))
mean(Spe(cbind(TN, FP, TP, FN)))
mean(Acc(cbind(TN, FP, TP, FN)))

result.final <- rbind(result.final, c("AdaSampling (logit)", "Akt", 
                  mean(F1(cbind(TN, FP, TP, FN))),
                  mean(Acc(cbind(TN, FP, TP, FN))),
                  mean(Sen(cbind(TN, FP, TP, FN))),
                  mean(Spe(cbind(TN, FP, TP, FN)))))


posLikeRatio = mean(Sen(cbind(TN, FP, TP, FN)))/(1-mean(Spe(cbind(TN, FP, TP, FN))))
paste("AdaSampling (logit) for.Akt - Positive Likelihood Ratio = ",posLikeRatio)

```

#### mTORSubstrate class
```{r Ada Sampling using logit for mTORSubstrate class}

mTORSubstrate.dat <- dat[c(-15)]

mTORSubstrate.dat$mTORSubstrate = as.numeric(as.factor(mTORSubstrate.dat$mTORSubstrate)) #TKP this does not make "0" into 0, and "1" into 1


## TKP code ##
mTORSubstrate.dat$mTORSubstrate_corrected = as.numeric(as.factor(mTORSubstrate.dat$mTORSubstrate)) - 1


## TKP code end ##
data.cls.truth <- mTORSubstrate.dat$mTORSubstrate_corrected
mTORSubstrate.dat <- mTORSubstrate.dat[,-c(15:16)]

dim(mTORSubstrate.dat)
```
```{r}

k = 10
set.seed(1)
fold=createFolds(data.cls.truth, k)
TP <- TN <- FP <- FN <- c()
for (i in 1:length(fold)){
  train.mat = mTORSubstrate.dat[-fold[[i]],-13]
  test.mat = mTORSubstrate.dat[fold[[i]],-13]
  cls = data.cls.truth[-fold[[i]]]
  
  #index positive and genative instances
  Ps = rownames(train.mat)[which(cls == 1)]
  Ns = rownames(train.mat)[which(cls == 0)]
  
  pred.prob = adaSample(Ps, Ns, train.mat, test.mat, classifier = "logit")
  pred = ifelse(pred.prob[, "P"] >0.5, 1, 0)
  
  TP <- c(TP, sum((data.cls.truth[fold[[i]]] == pred)[data.cls.truth[fold[[i]]] == "1"]))
  TN <- c(TN, sum((data.cls.truth[fold[[i]]] == pred)[data.cls.truth[fold[[i]]] == "0"]))
  FP <- c(FP, sum((data.cls.truth[fold[[i]]] != pred)[pred == "1"]))
  FN <- c(FN, sum((data.cls.truth[fold[[i]]] != pred)[pred == "0"]))
}

mean(F1(cbind(TN, FP, TP, FN)))
mean(Sen(cbind(TN, FP, TP, FN)))
mean(Spe(cbind(TN, FP, TP, FN)))
mean(Acc(cbind(TN, FP, TP, FN)))

# add to result.final
result.final <- rbind(result.final,c("AdaSampling (logit)", "mToR", 
                  mean(F1(cbind(TN, FP, TP, FN))),
                  mean(Acc(cbind(TN, FP, TP, FN))),
                  mean(Sen(cbind(TN, FP, TP, FN))),
                  mean(Spe(cbind(TN, FP, TP, FN)))))

posLikeRatio = mean(Sen(cbind(TN, FP, TP, FN)))/(1-mean(Spe(cbind(TN, FP, TP, FN))))
paste("AdaSampling (logit) for mToR - Positive Likelihood Ratio = ",posLikeRatio)


```


## 5.4 Ada Sampling using Random Forest

Prepare Akt substrate dataset for AdaSampling (mTOR substrate will have a different dataset)
```{r Prepare dataset for AdaSampling for Akt Substrate (mTOR substrate will have a different dataset)}
dat.extra.ada <- dat.extra %>% select(-Type) # Remove sequence window and type features.

# Explicitly store the Identifier into a separate matrix to reference back later.
rowNameToIdentifierMap <- data.frame(rownames(dat.extra.ada), dat.extra.ada$Identifier, dat.extra.ada$Seq.Window)

dat.extra.ada$Seq.Window <- NULL # Drop this factor window

names(rowNameToIdentifierMap) <- c("rowName", "Identifier", "Seq.Window")

dat.extra.ada$Identifier <- NULL # Remove Identifier column (each identifier is unique and not useful for classification)

# Save the positive (Akt) and negative (mTOR / unknown) classes to a separate dataframe for use in AdaSample. Postive and negative classes are indexed in the adaSampleWrapperRF fucntion.

dat.extra.ada.akt_cls <- dat.extra.ada$aktSubstrate
dat.extra.ada.mtor_cls <- dat.extra.ada$mTORSubstrate

# Drop the labels from the dat.extra.ada dataframe
dat.extra.ada$aktSubstrate <-NULL
dat.extra.ada$mTORSubstrate <-NULL

colnames(dat.extra.ada)
```
#### Akt Substrate
```{r Akt substrate prediction, ntree=800, mtry=25}
# 
# score <- adaSampleWrapperRF(dat = dat.extra.ada, cls = dat.extra.ada.mtor_cls, cls.known.neg = dat.extra.ada.akt_cls, folds = adafolds.mTOR, mtry=m, ntree=n)
# 
# trn.cls <- cls[-fold]
# tst.cls <- cls[fold] # need to use to estimate sensitivity
# 
# tst.cls.known.neg <- cls.known.neg[fold] # Need to use to estimate lower bound for specificity
# 
# train.mat <- dat[-fold,]
# test.mat <- dat[fold,]
# cls <- dat.extra.ada.mtor_cls

# Index positive and negative instances, this assumes all unlabeled samples are of the negative class
Ps <- rownames(dat.extra.ada)[which(dat.extra.ada.akt_cls == 1)]
Ns <- rownames(dat.extra.ada)[which(dat.extra.ada.akt_cls == 0)]

## Perform AdaSampling on the Training Set and provide prediction on the test set
# require(AdaSampling)
pred.prob <- adaSample(Ps, Ns, dat.extra.ada, test=dat.extra.ada, classifier="rf", C=50, mtry=25, ntree=800)

dim(pred.prob)
head(pred.prob[,])

sum(dat.extra.ada.akt_cls==1)


# summary of predictions
pred = ifelse(pred.prob[,"P"] > 0.5, 1, 0)

TP <- sum((dat.extra.ada.akt_cls == pred)[dat.extra.ada.akt_cls == "1"])
TN <- sum((dat.extra.ada.akt_cls == pred)[dat.extra.ada.akt_cls == "0"])
FP <- sum((dat.extra.ada.akt_cls != pred)[pred == "1"])
FN <- sum((dat.extra.ada.akt_cls != pred)[pred == "0"])

# add to result.final
result.final <- rbind(result.final,c("AdaSampling (RF)", "Akt", 
                  F1(cbind(TN, FP, TP, FN)),
                  Acc(cbind(TN, FP, TP, FN)),
                  Sen(cbind(TN, FP, TP, FN)),
                  Spe(cbind(TN, FP, TP, FN))))


#head(dat.extra.ada.akt_cls)

posLikeRatio = Sen(cbind(TN, FP, TP, FN))/(1-Spe(cbind(TN, FP, TP, FN)))
paste("AdaSampling (Final) - Positive Likelihood Ratio = ",posLikeRatio)
```

#### mTOR predictions

```{r mTOR substrate prediction, ntree=100, mtry=32}

Ps <- rownames(dat.extra.ada)[which(dat.extra.ada.mtor_cls == 1)]
Ns <- rownames(dat.extra.ada)[which(dat.extra.ada.mtor_cls == 0)]

## Perform AdaSampling on the Training Set and provide prediction on the test set
# require(AdaSampling)
pred.prob.mTOR <- adaSample(Ps, Ns, dat.extra.ada, test=dat.extra.ada, classifier="rf", C=50, mtry=32, ntree=100)

dim(pred.prob.mTOR)
head(pred.prob.mTOR)

# summary of predictions
pred = ifelse(pred.prob.mTOR[,"P"] > 0.5, 1, 0)

TP <- sum((dat.extra.ada.mtor_cls == pred)[dat.extra.ada.mtor_cls == "1"])
TN <- sum((dat.extra.ada.mtor_cls == pred)[dat.extra.ada.mtor_cls == "0"])
FP <- sum((dat.extra.ada.mtor_cls != pred)[pred == "1"])
FN <- sum((dat.extra.ada.mtor_cls != pred)[pred == "0"])

# add to result.final
result.final <- rbind(result.final,c("AdaSampling (RF)", "mToR", 
                  F1(cbind(TN, FP, TP, FN)),
                  Acc(cbind(TN, FP, TP, FN)),
                  Sen(cbind(TN, FP, TP, FN)),
                  Spe(cbind(TN, FP, TP, FN))))


sum(dat.extra.ada.mtor_cls==1)


head(dat.extra.ada.mtor_cls)

```

# 6. Validation and benchmark of prediction results
Compare predictions to those of the 2016 study.
Use the best identified cost and gamma values for generating a final adasample on the whole dataset using an SVM as the base classifier inside the AdaSampling method.

## 6.1 Compare the Ada Sampling methods for both Akt and mTOR predictions
```{r compare the results.final}
# write the results into csv

result.final <- data.frame(result.final)
names(result.final) <- c("Method", "Substrate","F1","Accuracy","Sensitivity", "Specificity")

result.final <- transform(result.final, F1 = as.numeric(levels(F1))[F1],
                                        Accuracy = as.numeric(levels(Accuracy))[Accuracy],
                                        Sensitivity = as.numeric(levels(Sensitivity))[Sensitivity],
                                        Specificity = as.numeric(levels(Specificity))[Specificity])


write.csv(result.final, './Output/result.final.csv')

# print the results for comparision
result.final

```

## 6.2 Generate final predictions on the unlabeled dataset using the best identified classifier from prior experiments

```{r}
AktFinalPred <- data.frame(pred.prob,dat.extra.ada.akt_cls) %>% merge(rowNameToIdentifierMap, by.x="row.names", by.y="rowName") %>%arrange(desc(P)) %>% filter(dat.extra.ada.akt_cls == 0) 

mTORFinalPred <- data.frame(pred.prob.mTOR,dat.extra.ada.mtor_cls) %>% merge(rowNameToIdentifierMap, by.x="row.names", by.y="rowName") %>%arrange(desc(P)) %>% filter(dat.extra.ada.mtor_cls == 0) 

head(AktFinalPred %>% arrange(Row.names))
head(mTORFinalPred %>% arrange(Row.names))

```

```{r}
# Combine the results
final.pred.combined <- AktFinalPred %>% rename(AktProb = P) %>% select(Row.names, AktProb) %>% merge(mTORFinalPred %>% rename(mTORProb = P) %>% select(Row.names, mTORProb, Identifier, Seq.Window), by="Row.names") %>%
mutate(AktPred = ifelse(AktProb > mTORProb & AktProb > 0.5, 1,0),
       mTORPred = ifelse(mTORProb > AktProb & mTORProb > 0.5, 1,0))

head(final.pred.combined)

```


## 6.3 Compare the model predictions to the predictions of academic paper
```{r }


Prediction2016.akt <- readxl::read_excel("./Data2/Prediction_2016.xlsx", sheet="Akt_prediction")
Prediction2016.akt <- Prediction2016.akt %>% mutate(Identifier=paste(str_to_upper(GeneSymbol),";" ,`Phosphorylation site`, ';', sep=""))

head(Prediction2016.akt)

Prediction2016.mtor <- readxl::read_excel("./Data2/Prediction_2016.xlsx", sheet="mTOR_prediction")
Prediction2016.mtor <- Prediction2016.mtor %>% mutate(Identifier=paste(str_to_upper(GeneSymbol),";" ,`Phosphorylation site`, ';', sep=""))

head(Prediction2016.akt)
head(Prediction2016.mtor)

dim(Prediction2016.akt)
dim(Prediction2016.mtor)
```

## 6.4 Benchmark of prediction results

### 6.4.1 Sensitivity
```{r}

p.Sen <- ggplot(result.final) +
  geom_col(aes(x=Method, y=Sensitivity, color=Method, fill=Method)) +
  geom_text(data=result.final,aes(x=Method,y=Sensitivity, label=as.character(round(Sensitivity,2))),vjust=1.2, size=3) +
  xlab("Method") +
  ylab("Avg. Sensitivity over 10 Folds") +
  scale_y_continuous(labels = percent) +
  facet_wrap(~Substrate) +
  ggtitle("AdaSample Classifiers - Sensitivity ") + 
  theme_light() +
  theme(axis.title.x=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank())

p.Sen

```
The sensitivity plot results above show the average sensitivity of the AdaSampled classifiers using 10 folds of cross validation to form the estimate. From the column chart plot we can see that using SVM as the base classifier out performs all other methods in both prediction of Akt and mTOR substrates. Note: This sensitivity measure is based only on the known labels of Akt and mTOR, there are potential unlabeled positives that are unaccounted for in the statistics above.

### 6.4.2 Specificity (lower bound)
```{r}


p.Spe <- ggplot(result.final) +
  geom_col(aes(x=Method, y=Specificity, color=Method, fill=Method)) +
  geom_text(data=result.final,aes(x=Method,y=Specificity, label=as.character(round(Specificity,2))),vjust=1.2, size=3) +
  xlab("Method") +
  ylab("Avg. Specificity over 10 Folds") +
  scale_y_continuous(labels = percent) +
  facet_wrap(~Substrate) +
  ggtitle("AdaSample Classifiers - Specificity (lower bd) ") + 
  theme_light() +
  theme(axis.title.x=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank())

p.Spe

```

The specificity plot results above show the average specificity of the AdaSampled classifiers using 10 folds of cross validation to form the estimate. From the column chart plot we can see that using SVM as the base classifier out performs all other methods in both prediction of Akt and mTOR substrates (with only a small margin between RF and SVM for Akt substrate prediction). Note: This specificity measure is assumes all unlabeled samples are of the negative class for both Akt and mTOR. As a result, the specificity measured in this statistic is a 'lower bound' measurement of specificity as there are potential positive samples in the unknown set.

## 6.5 Generate required outputs
(1) Probability of each sample been mislabelled; and
(2) A predicted list of mislabelled samples from training and test dataset, respectively.

```{r Combined - Join the two tables on the Identifier field}
head(final.pred.combined)

# (1) Probability of each sample been mislabelled; and
write.csv(final.pred.combined, "./Output/Akt_mTOR_Prediction_Probabilities.csv")

# (2) A predicted list of mislabelled samples from training and test dataset, respectively.
## (2a - Akt)
write.csv(final.pred.combined %>% filter(AktPred == 1)%>% select(Identifier,Seq.Window, AktProb, AktPred), "./Output/Akt_Prediction_List.csv")

## (2b - mTOR)
write.csv(final.pred.combined %>% filter(mTORPred == 1) %>% select(Identifier,Seq.Window, mTORProb, mTORPred), "./Output/mTOR_Prediction_List.csv")


```


## 6.6 Compare predictions with Prediction2016.xlsx predictions
Note, the two datasets AktFinalPred and Prediction2016 appear to have different gene symbol, sites, although there is an intersect between the two.


```{r Join the Prediction2016.xlsx tables on the Identifier field and Sequence Window field}

# Akt
CompareFinalPred <- final.pred.combined %>% merge(Prediction2016.akt%>% dplyr::select(Identifier, `Sequence window`, `Full model predict`) %>% mutate(pred.2016.Akt = ifelse(`Full model predict` > 0.5, 1,0)), by.x=c("Identifier","Seq.Window"), by.y=c("Identifier", "Sequence window"), all.x=T, all.y=F) %>% rename(pred.2016.Akt.prob = `Full model predict`)


# mTOR
CompareFinalPred <- CompareFinalPred %>% merge(Prediction2016.mtor%>% dplyr::select(Identifier, `Sequence window`, `Full model predict`) %>% mutate(pred.2016.mtor = ifelse(`Full model predict` > 0.5, 1,0)), by.x=c("Identifier","Seq.Window"), by.y=c("Identifier", "Sequence window"), all.x=T, all.y=F) %>% rename(pred.2016.mTOR.prob = `Full model predict`)

CompareFinalPred$Row.names <- NULL

names(CompareFinalPred)
head(CompareFinalPred)


```

```{r Compare results for Akt where prediction was made in 2016}
## CREDIT: https://ragrawal.wordpress.com/2011/05/16/visualizing-confusion-matrix-in-r/
# Code was adapted from URL above.
# Author: Ritesh Agarwal (2011)

#CompareFinalPred[which(!is.na(CompareFinalPred$pred.2016.Akt)),"pred.2016.Akt"]

#compute frequency of actual categories
actual.Akt = as.data.frame(table(CompareFinalPred[which(!is.na(CompareFinalPred$pred.2016.Akt)),"pred.2016.Akt"]))
names(actual.Akt) = c("Actual","ActualFreq")

#compute confusion matrix
confusion.Akt = as.data.frame(table(CompareFinalPred[which(!is.na(CompareFinalPred$pred.2016.Akt)),"pred.2016.Akt"], CompareFinalPred[which(!is.na(CompareFinalPred$pred.2016.Akt)),"AktPred"]))

names(confusion.Akt) = c("Actual","Predicted","Freq")

# confusion.Akt

# head(confusion.Akt)
# head(actual.Akt)

#calculate percentage of test cases based on actual frequency
confusion.Akt = merge(confusion.Akt, actual.Akt, by=c("Actual"))
confusion.Akt$Percent = confusion.Akt$Freq/confusion.Akt$ActualFreq*100
 
#render plot
# we use three different layers
# first we draw tiles and fill color based on percentage of test cases
tile.Akt <- ggplot() +
geom_tile(aes(x=Actual, y=Predicted,fill=Percent),data=confusion.Akt, color="black",size=0.1) +
labs(x="2016 Prediction",y="SVM Model Prediction") +
  #scale_x_discrete(limits = c(1,0), position = 'top') +
    theme(axis.text.x.top = element_text(angle = 0, vjust=0.5, hjust=0)) +
  ggtitle("Akt Prediction comparison to Prediction2016.xlsx")

tile.Akt = tile.Akt + 
geom_text(aes(x=Actual,y=Predicted, label=sprintf("%1.f (%.1f%%)", Freq, Percent)),data=confusion.Akt, size=3, colour="black", nudge_x = +0.0) +

scale_fill_gradientn(colours = rev(heat.colors(10)))
 
# lastly we draw diagonal tiles. We use alpha = 0 so as not to hide previous layers but use size=0.3 to highlight border
tile.Akt = tile.Akt + 
geom_tile(aes(x=Actual,y=Predicted),data=subset(confusion.Akt, as.character(Actual)==as.character(Predicted)), color="black",size=0.3, fill="black", alpha=0) 
 
#render
tile.Akt

```

From the pseudo confusion matrix above comparing predictions made by the 


```{r Compare results for mTOR where prediction was made in 2016}
## CREDIT: https://ragrawal.wordpress.com/2011/05/16/visualizing-confusion-matrix-in-r/
# Code was adapted from URL above.
# Author: Ritesh Agarwal (2011)

#CompareFinalPred[which(!is.na(CompareFinalPred$pred.2016.mTOR)),"pred.2016.mTOR"]

#compute frequency of actual categories
actual.mTOR = as.data.frame(table(CompareFinalPred[which(!is.na(CompareFinalPred$pred.2016.mtor)),"pred.2016.mtor"]))
names(actual.mTOR) = c("Actual","ActualFreq")

#compute confusion matrix
confusion.mTOR = as.data.frame(table(CompareFinalPred[which(!is.na(CompareFinalPred$pred.2016.mtor)),"pred.2016.mtor"], CompareFinalPred[which(!is.na(CompareFinalPred$pred.2016.mtor)),"mTORPred"]))

names(confusion.mTOR) = c("Actual","Predicted","Freq")

# confusion.mTOR

# head(confusion.mTOR)
# head(actual.mTOR)

#calculate percentage of test cases based on actual frequency
confusion.mTOR = merge(confusion.mTOR, actual.mTOR, by=c("Actual"))
confusion.mTOR$Percent = confusion.mTOR$Freq/confusion.mTOR$ActualFreq*100
 
#render plot
# we use three different layers
# first we draw tiles and fill color based on percentage of test cases
tile.mTOR <- ggplot() +
geom_tile(aes(x=Actual, y=Predicted,fill=Percent),data=confusion.mTOR, color="black",size=0.1) +
labs(x="2016 Prediction",y="SVM Model Prediction") +
  #scale_x_discrete(limits = c(1,0), breaks=c("1","0"), position = 'top') +
    theme(axis.text.x.top = element_text(angle = 0, vjust=0.5, hjust=0)) +
  ggtitle("mTOR Prediction comparison to Prediction2016.xlsx")

tile.mTOR = tile.mTOR + 
geom_text(aes(x=Actual,y=Predicted, label=sprintf("%1.f (%.1f%%)", Freq, Percent)),data=confusion.mTOR, size=3, colour="black", nudge_x = +0.0) +
scale_fill_gradientn(colours = rev(heat.colors(10)))
 
# lastly we draw diagonal tiles. We use alpha = 0 so as not to hide previous layers but use size=0.3 to highlight border
tile.mTOR = tile.mTOR + 
geom_tile(aes(x=Actual,y=Predicted),data=subset(confusion.mTOR, as.character(Actual)==as.character(Predicted)), color="black",size=0.3, fill="black", alpha=0) 
 
#render
tile.mTOR

```


# 7. Conclusion
Based on the experimentation and interpretation of results, AdaSampling using random forest as a base classifier is epxected to perform best for novel substrate Akt and mTOR identified. This model will be selected as final classification for Kinase-substrate prediction due to following reasons,<br/>
1) Adaptive Sampling handles the positive labelled data well<br/>
2) When unlabeled positive labels are accounted for by (1), random forest is robust to class imbalance<br/>
3) Sensitivity and specificity is higher among all models when AdaSampling is applied compared to without<br/>
4) The RF AdaSample and Prediction 2016 results are more alike for mTOR substrate predictions than Akt substrate predictions for positive samples, based on confusion matrix agreement of (1 (RF AdaSample) and 1 (Prediction 2016))
5) For negative labeling the predictions are much more closely aligned (0 (RF AdaSample) and 0 (Prediction 2016))

# 8. Future Works
To truly understand the causes of the underlying difference in prediction, we must investigate in detail samples where the prediction varied. A code comparison to the experiments carried out to produce the "Prediction_2016.xlsx" results needs to be undertaken to identify potential areas where differences occurred, including feature generation, selection and classifier parameters used.


  